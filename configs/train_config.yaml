# Model & Data
model:
  weights: "yolov8l.pt"
  imgsz: 640

data:
  config: "data.yaml"
  workers: 8
  batch_size: 4

# Training Schedule
training:
  epochs: 100
  warmup_epochs: 10               # Warmup period (no distillation loss)
  lr0: 0.0001                     # Initial learning rate
  lrf: 0.01                       # Final LR multiplier
  device: "0"

# Mean Teacher EMA
# Formula: θ_teacher = α * θ_teacher + (1-α) * θ_student
# Only PARAMETERS updated, BUFFERS (BatchNorm) stay frozen
# Teacher always in eval mode, update called AFTER optimizer.step()
teacher:
  freeze_teacher: false           # If true, no EMA updates
  alpha: 0.999                    # Higher = slower adaptation

# Distillation (Teacher → Student knowledge transfer)
distillation:
  conf_thres_min: 0.5             # Curriculum: start low
  conf_thres_max: 0.7             # Curriculum: end high
  iou_thres: 0.45
  lambda_weight: 0.1              # Distillation loss weight
  use_progressive_lambda: true
  # Class mapping: COCO ID → Dataset ID
  class_mapping:
    0: 0    # person
    2: 1    # car

# Domain Adversarial (GRL)
# Gradient Reversal Layer aligns source/target feature distributions
grl:
  enabled: true
  warmup_epochs: 10
  max_alpha: 0.3
  weight: 0.05
  hidden_dim: 256
  dropout: 0.3
  lr: 0.0001

# Output
output:
  project: "runs/fda"
  name: "exp"

logging:
  log_interval: 100
  val_interval: 10
  save_interval: 10
  enable_monitoring: true

# Performance
performance:
  amp: true
  cache_clear_interval: 200
  gradient_clip: 2.0
